{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage.feature as sf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import heapq  # Import heapq for getting the smallest n elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SSD MobileNet v2 model\n",
    "def load_ssd_model(model_path):\n",
    "    # Load the saved model\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying region based segementation\n",
    "\n",
    "def region_based_segmentation(image):\n",
    "    # Load the image in color\n",
    "    original_image = image.copy()  # Make a copy of the input image for segmentation\n",
    "    if original_image is None:\n",
    "        raise ValueError(\"Image could not be loaded.\")\n",
    "\n",
    "    # Convert the image to grayscale for thresholding\n",
    "    gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to obtain binary image\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Perform morphological transformations to enhance the image\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    # Find sure regions (where the foreground is clearly visible)\n",
    "    dist_transform = cv2.distanceTransform(dilated, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # Find sure background (where the background is clearly visible)\n",
    "    sure_bg = cv2.dilate(thresh, kernel, iterations=3)\n",
    "\n",
    "    # Ensure sure_bg and sure_fg are the same data type (np.uint8)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    sure_bg = np.uint8(sure_bg)\n",
    "\n",
    "    # Subtract sure background from sure foreground\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    # Marker labelling for Watershed\n",
    "    _, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "    # Add one to all markers to distinguish background\n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    # Apply Watershed algorithm\n",
    "    segmented_image = original_image.copy()\n",
    "    cv2.watershed(segmented_image, markers)\n",
    "\n",
    "    # Mark boundaries in red\n",
    "    segmented_image[markers == -1] = [0, 0, 255]\n",
    "\n",
    "    return segmented_image, original_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying canny edge detection\n",
    "\n",
    "def canny_edge_detection(image, low_threshold=50, high_threshold=150):\n",
    "   \n",
    "   original_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "   edges = cv2.Canny(original_grayscale, low_threshold, high_threshold)\n",
    "   return edges, original_grayscale\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlaying of images\n",
    "\n",
    "def overlay_edges_on_image(original_image, edges):\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    # Ensure the edges are visible (overlay them with some transparency on the original image)\n",
    "    output_image = cv2.addWeighted(original_image, 0.7, edges_colored, 0.3, 0)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region bases segmentaion and canny edge detection\n",
    "\n",
    "def regcanny(image):\n",
    "         # Step 1: Perform region-based segmentation\n",
    "        segmented_image, original_image = region_based_segmentation(image)\n",
    "\n",
    "        # Step 2: Perform Canny edge detection\n",
    "        edges, original_grayscale = canny_edge_detection(image, low_threshold=50, high_threshold=150)\n",
    "\n",
    "        # Step 3: Overlay edges on the original image\n",
    "        output_image = overlay_edges_on_image(original_image, edges)\n",
    "\n",
    "        # Step 4: Display the results\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Display the segmented image with boundaries\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Region-Based Segmentation\")\n",
    "        plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Display the original grayscale image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Canny Edge Detection\")\n",
    "        plt.imshow(edges, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Display the overlay image with edges on the original\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Overlayed Edges on Original\")\n",
    "        plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Capture Image from Webcam\n",
    "def capture_image():\n",
    "    \"\"\"Capture an image from the webcam and save it.\"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Press 's' to save the image or 'q' to quit.\")\n",
    "    captured_image = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture image. Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Display the live video feed\n",
    "        cv2.imshow(\"Webcam Feed\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):  # Save the image\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            file_path = f\"captured_images/captured_image_{timestamp}.jpg\"\n",
    "            cv2.imwrite(file_path, frame)\n",
    "            print(f\"Image saved at {file_path}\")\n",
    "            captured_image = frame\n",
    "            break\n",
    "        elif key == ord('q'):  # Quit without saving\n",
    "            print(\"Exiting without capturing.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return captured_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"Resize and normalize the image for SSD MobileNet.\"\"\"\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
    "    image_resized = cv2.resize(image_rgb, (320, 320))    # Resize to 320x320 (required for SSD MobileNet)\n",
    "    image_normalized = np.expand_dims(image_resized, axis=0).astype(np.float32)  # Expand dims for batch size\n",
    "    image_normalized = image_normalized / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Detect Objects with SSD\n",
    "def detect_objects(model, image):\n",
    "    \"\"\"Detect objects using SSD MobileNet model.\"\"\"\n",
    "    input_tensor = preprocess_image(image)\n",
    "    # Run inference\n",
    "    detections = model(input_tensor)\n",
    "    print(detections)\n",
    "\n",
    "    # Get detection boxes, scores, and class indices\n",
    "    boxes = detections['detection_boxes'][0].numpy()  # Adjust based on actual model output\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    class_indices = detections['detection_classes'][0].numpy().astype(np.int32)\n",
    "\n",
    "    return boxes, scores, class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_objects(image, boxes, class_indices, scores):\n",
    "    # Ensure scores are in a NumPy array format (in case they are tensors)\n",
    "    if isinstance(scores, tf.Tensor):\n",
    "        scores = scores.numpy()  # Convert to numpy array if TensorFlow tensor\n",
    "\n",
    "    # Check if scores is a 2D array (e.g., (num_boxes, num_classes)) and select the highest score\n",
    "    if scores.ndim == 2:\n",
    "        # Select the highest score per box (e.g., for multi-class detections)\n",
    "        scores = np.max(scores, axis=-1)\n",
    "\n",
    "    # Loop through each detection\n",
    "    for i in range(len(scores)):\n",
    "        score = scores[i]\n",
    "\n",
    "        # Check if the score is greater than 0.5 (ensure score is a scalar)\n",
    "        if score > 0.5:\n",
    "            box = boxes[i]\n",
    "            class_id = class_indices[i]\n",
    "\n",
    "            # Get the coordinates of the bounding box (normalized to image size)\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            xmin, xmax, ymin, ymax = int(xmin * image.shape[1]), int(xmax * image.shape[1]), int(ymin * image.shape[0]), int(ymax * image.shape[0])\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "            # Label the class ID and score\n",
    "            label = f\"Class: {class_id}, Score: {score:.2f}\"\n",
    "            cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def generate_grasp_points(boxes):\n",
    "    \"\"\"Generate grasp points based on object centers and orientation.\"\"\"\n",
    "    grasp_points = []\n",
    "    for box in boxes:\n",
    "        print(box)\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        # Calculate the center of the bounding box\n",
    "        center_x = (xmin + xmax) / 2\n",
    "        center_y = (ymin + ymax) / 2\n",
    "        # Assign an arbitrary angle (you can modify this logic as needed)\n",
    "        angle = 0  # Assuming the grasping angle is 0 for simplicity\n",
    "        grasp_points.append({'center': (center_x, center_y), 'angle': angle})\n",
    "    return grasp_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Draw Grasp Points\n",
    "def draw_grasp_points(image, grasp_points):\n",
    "    print(\"i am inside draw_grasp_points\")\n",
    "    \"\"\"Draw grasp rectangles at grasp points.\"\"\"\n",
    "    print(\".....\")\n",
    "    print(grasp_points)\n",
    "    print(\".....\")\n",
    "    for point in grasp_points:\n",
    "        center = tuple(map(int, point['center']))\n",
    "        angle = 0\n",
    "        width, height = 20, 20  # Example grasp dimensions\n",
    "        box = cv2.boxPoints(((center[0], center[1]), (width, height), angle))\n",
    "        box = np.intp(box)\n",
    "        cv2.drawContours(image, [box], -1, (255, 0, 0), 2)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_grasp_to_pixel(image, grasp_point):\n",
    "  \n",
    "    # Get the image dimensions\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    print(type(grasp_point))\n",
    "    print(\"grasp point inside convert grast to poizel\")\n",
    "    print(grasp_point)\n",
    "    # Convert the grasp center from normalized coordinates to pixel values\n",
    "    normalized_center = grasp_point['center']\n",
    "    center_x_pixel = int(normalized_center[0] * image_width)\n",
    "    center_y_pixel = int(normalized_center[1] * image_height)\n",
    "\n",
    "    # The angle remains unchanged (if it's 0, no conversion needed)\n",
    "    angle = grasp_point['angle']\n",
    "\n",
    "    # Return the grasp point in pixel values\n",
    "    grasp_point_pixel = {\n",
    "        'center': (center_x_pixel, center_y_pixel),\n",
    "        'angle': angle\n",
    "    }\n",
    "\n",
    "    return grasp_point_pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert grasp point from normalized coordinates to pixel values\n",
    "def convert_grasp_to_pixel(image, grasp_point):\n",
    "    # Get the image dimensions\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Convert the grasp center from normalized coordinates to pixel values\n",
    "    normalized_center = grasp_point[0]\n",
    "    center_x_pixel = int(normalized_center[0] * image_width)\n",
    "    center_y_pixel = int(normalized_center[1] * image_height)\n",
    "\n",
    "    # The angle remains unchanged (if it's 0, no conversion needed)\n",
    "    angle = grasp_point[1]\n",
    "\n",
    "    # Return the grasp point in pixel values\n",
    "    grasp_point_pixel = [[(center_x_pixel, center_y_pixel),angle]]\n",
    "    return grasp_point_pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_image_with_grasp(image, grasp_points):\n",
    "  \n",
    "    image_copy = image.copy()\n",
    "    for grasp_point in grasp_points:\n",
    "        # Convert the grasp center to pixel values\n",
    "        center = grasp_point[0]\n",
    "        angle = grasp_point[1]\n",
    "\n",
    "        # Draw the grasp point as a circle on the image\n",
    "        cv2.circle(image_copy, center, 10, (0, 0, 255), -1)  # Red circle at grasp point\n",
    "\n",
    "        # Optionally, draw the grasp rectangle (for visualization of grasp angle)\n",
    "        width, height = 50, 20  # Arbitrary dimensions for the grasp rectangle\n",
    "        box = cv2.boxPoints(((center[0], center[1]), (width, height), angle))\n",
    "        box = np.intp(box)\n",
    "        cv2.drawContours(image_copy, [box], 0, (0, 255, 0), 2)  # Green grasp rectangle\n",
    "    \n",
    "    plt.title(\"possible grasp points\")\n",
    "    plt.imshow(cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')  # Hide axes for better visualization\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folder(folder_path, model):\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png'))]\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"Failed to load image {image_file}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        inpimage = regcanny(image)\n",
    "       # Detect objects in the image\n",
    "        boxes, scores, class_indices = detect_objects(model, inpimage) \n",
    "\n",
    "        # Draw detected objects\n",
    "        draw_objects(inpimage.copy(), boxes, class_indices, scores)\n",
    "        # Generate grasp points based on detected objects\n",
    "        grasp_points = generate_grasp_points(boxes)\n",
    "        # Assuming grasp_points is a list of dictionaries with 'center' and 'angle'\n",
    "        for i in grasp_points[:20]:\n",
    "            grasp_points_pixel = convert_grasp_to_pixel(inpimage.copy(), list(i.values()))\n",
    "            plot_image_with_grasp(inpimage, grasp_points_pixel)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"captured_images\", exist_ok=True)\n",
    "    \n",
    "    # Load the model (provide path to the saved model)\n",
    "    model_path = 'ssd-mobilenet-v2-tensorflow2-fpnlite-320x320-v1'\n",
    "    model = load_ssd_model(model_path)\n",
    "    
    "    folder_path = '--folder path --",
    "    image = capture_image()\n",
    "    if image is None:\n",
    "        exit()\n",
    "    process_images_in_folder(folder_path, model)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
